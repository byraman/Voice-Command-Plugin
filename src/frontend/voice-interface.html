<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Interface - Figma Plugin</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            margin: 0;
            padding: 20px;
            background: #f8f9fa;
            text-align: center;
        }
        .container {
            max-width: 400px;
            margin: 0 auto;
        }
        .title {
            color: #333;
            margin-bottom: 30px;
        }
        .info-message {
            background: #e3f2fd;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            border-left: 4px solid #2196f3;
            font-size: 14px;
            color: #1565c0;
            text-align: left;
        }
        .transcript {
            background: white;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            min-height: 60px;
            border: 2px solid #e1e5e9;
            font-size: 16px;
            color: #333;
            text-align: left;
        }
        .status {
            color: #666;
            font-size: 14px;
            margin: 10px 0;
        }
        .mic-button {
            background: #007AFF;
            color: white;
            border: none;
            padding: 15px 30px;
            border-radius: 25px;
            cursor: pointer;
            font-size: 16px;
            margin: 20px 0;
            transition: all 0.3s ease;
            box-shadow: 0 4px 15px rgba(0, 122, 255, 0.3);
        }
        .mic-button:hover {
            background: #0056CC;
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(0, 122, 255, 0.4);
        }
        .mic-button:disabled {
            background: #ccc;
            cursor: not-allowed;
            transform: none;
            box-shadow: none;
        }
        .mic-button.listening {
            background: #FF3B30;
            animation: pulse 1.5s infinite;
        }
        @keyframes pulse {
            0% { transform: scale(1); }
            50% { transform: scale(1.05); }
            100% { transform: scale(1); }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1 class="title">ðŸŽ¤ Voice Interface</h1>
        
        <div class="info-message">
            <strong>The webpage is just used as a mic source since Figma doesn't support audio input natively.</strong>
        </div>
        
        <button id="micButton" class="mic-button">
            ðŸŽ¤ Start Listening
        </button>
        
        <div id="transcript" class="transcript">
            Waiting for voice input...
        </div>
        
        <div id="status" class="status">
            Click the button to start voice recognition
        </div>
    </div>

    <script src="../config.js"></script>
    <script>
        class VoiceInterface {
            constructor() {
                this.isListening = false;
                this.recognition = null;
                this.silenceTimer = null;
                this.micButton = document.getElementById('micButton');
                this.transcript = document.getElementById('transcript');
                this.status = document.getElementById('status');
                
                this.init();
            }
            
            init() {
                this.setupSpeechRecognition();
                this.setupMicButton();
                this.setupMessageListener();
            }
            
            setupMicButton() {
                this.micButton.addEventListener('click', () => {
                    this.toggleListening();
                });
            }
            
            setupMessageListener() {
                window.addEventListener('message', (event) => {
                    console.log('Message received from Figma plugin:', event.data);
                    this.handlePluginMessage(event.data);
                });
                
                // Send ready signal to plugin
                setTimeout(() => {
                    this.sendToPlugin({ type: 'ready' });
                }, 1000);
            }
            
            handlePluginMessage(data) {
                switch (data.type) {
                    case 'start':
                        this.startRecognition();
                        break;
                    case 'stop':
                        this.stopRecognition();
                        break;
                    case 'ready':
                        this.updateStatus('Connected to Figma plugin');
                        break;
                }
            }
            
            setupSpeechRecognition() {
                if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
                    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                    this.recognition = new SpeechRecognition();
                    
                    this.recognition.continuous = true;
                    this.recognition.interimResults = true;
                    this.recognition.lang = 'en-US';
                    
                    this.recognition.onstart = () => {
                        console.log('Speech recognition started');
                        this.isListening = true;
                        this.updateUI();
                        this.updateStatus('Listening... Speak your command');
                    };
                    
                    this.recognition.onresult = (event) => {
                        let finalTranscript = '';
                        let interimTranscript = '';
                        
                        for (let i = event.resultIndex; i < event.results.length; i++) {
                            const transcript = event.results[i][0].transcript;
                            if (event.results[i].isFinal) {
                                finalTranscript += transcript;
                            } else {
                                interimTranscript += transcript;
                            }
                        }
                        
                        const displayText = finalTranscript || interimTranscript;
                        this.transcript.textContent = displayText;
                        
                        if (finalTranscript) {
                            console.log('ðŸŽ¤ Final transcript:', finalTranscript);
                            
                            // Send command to the server
                            this.sendCommand(finalTranscript);
                            
                            this.resetSilenceTimer();
                        } else if (interimTranscript.trim().length > 0) {
                            // Only reset timer if there's actual speech content
                            this.resetSilenceTimer();
                        }
                    };
                    
                    this.recognition.onend = () => {
                        console.log('Speech recognition ended');
                        this.isListening = false;
                        this.updateUI();
                        this.updateStatus('Stopped listening');
                    };
                    
                    this.recognition.onerror = (event) => {
                        console.error('Speech recognition error:', event.error);
                        this.isListening = false;
                        this.updateStatus('Error: ' + event.error);
                        
                        if (event.error === 'not-allowed') {
                            this.updateStatus('Microphone access denied. Please allow microphone access and try again.');
                        } else if (event.error === 'no-speech') {
                            this.updateStatus('No speech detected. Please try again.');
                        } else if (event.error === 'audio-capture') {
                            this.updateStatus('No microphone found. Please check your microphone.');
                        } else if (event.error === 'network') {
                            this.updateStatus('Network error. Voice recognition needs HTTPS.');
                        }
                    };
                } else {
                    this.updateStatus('Speech recognition not supported in this browser');
                }
            }
            
            toggleListening() {
                if (this.isListening) {
                    this.stopRecognition();
                } else {
                    this.startRecognition();
                }
            }
            
            startRecognition() {
                if (this.recognition && !this.isListening) {
                    console.log('Starting speech recognition...');
                    this.recognition.start();
                    this.resetSilenceTimer();
                }
            }
            
            stopRecognition() {
                if (this.recognition && this.isListening) {
                    console.log('Stopping speech recognition...');
                    this.recognition.stop();
                    this.clearSilenceTimer();
                }
            }
            
            resetSilenceTimer() {
                this.clearSilenceTimer();
                this.silenceTimer = setTimeout(() => {
                    console.log('Starting silence timer (5 seconds)');
                    if (this.isListening) {
                        console.log('Silence detected, stopping recognition...');
                        this.recognition.stop();
                    }
                }, 5000);
            }
            
            clearSilenceTimer() {
                if (this.silenceTimer) {
                    clearTimeout(this.silenceTimer);
                    this.silenceTimer = null;
                }
            }
            
            async sendCommand(command) {
                try {
                    // Use Vercel URL
                    const apiUrl = window.API_BASE_URL || 'https://voice-command-plugin.vercel.app';
                    
                    const response = await fetch(`${apiUrl}/api/commands`, {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json',
                        },
                        body: JSON.stringify({ transcript: command.trim() })
                    });
                    
                    if (response.ok) {
                        this.updateStatus(`Command sent: "${command}"`);
                        this.transcript.textContent = 'Command sent! Waiting for next command...';
                        
                        // Also send to plugin via postMessage
                        this.sendToPlugin({ type: 'transcript', transcript: command.trim() });
                    }
                } catch (error) {
                    console.error('Error sending command:', error);
                    this.updateStatus('Error sending command');
                }
            }
            
            sendToPlugin(data) {
                // Try to send to parent window (Figma plugin)
                if (window.opener && !window.opener.closed) {
                    window.opener.postMessage(data, '*');
                    console.log('âœ… Sent to Figma plugin via opener');
                } else {
                    console.log('âŒ No Figma plugin window available');
                }
            }
            
            updateUI() {
                if (this.isListening) {
                    this.micButton.classList.add('listening');
                    this.micButton.textContent = 'ðŸ”´ Stop Listening';
                } else {
                    this.micButton.classList.remove('listening');
                    this.micButton.textContent = 'ðŸŽ¤ Start Listening';
                }
            }
            
            updateStatus(message) {
                this.status.textContent = message;
            }
        }
        
        // Initialize the voice interface
        document.addEventListener('DOMContentLoaded', () => {
            new VoiceInterface();
        });
    </script>
</body>
</html>